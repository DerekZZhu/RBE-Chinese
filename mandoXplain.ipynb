{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DistilBertModel\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from google.colab import drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HateSpeechExample:\n",
    "  text: str\n",
    "  label: int\n",
    "  rules : list\n",
    "  exemplars: str\n",
    "\n",
    "  @staticmethod\n",
    "  def from_list(sample):\n",
    "    text, label = sample[0], sample[1]\n",
    "\n",
    "    rules = []\n",
    "    exemplars = \"\"\n",
    "    if label == \"normal\":\n",
    "      label = 0\n",
    "    else:\n",
    "      label = 1\n",
    "    return HateSpeechExample(text, label, rules, exemplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class HateSpeechDataset(Dataset):\n",
    "  tokenizer = None\n",
    "\n",
    "  def __init__(self, raw_data_list, tokenizer, rules, exemplars):\n",
    "    HateSpeechDataset.tokenizer = tokenizer\n",
    "    self.data = [HateSpeechExample.from_list(sample) for sample in raw_data_list]\n",
    "    self.rules = rules\n",
    "    self.exemplars = exemplars\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def find_rules(self, idx):\n",
    "    text = self.data[idx].text\n",
    "    applied_rules = []\n",
    "    exemplars=\"\"\n",
    "    for rule in self.rules:\n",
    "      if text.find(rule)!=-1:\n",
    "        applied_rules.append(rule)\n",
    "        rule_exemplars = \" \".join(self.exemplars[rule])\n",
    "        exemplars += \" \" + rule_exemplars\n",
    "\n",
    "      if len(applied_rules) == 0:\n",
    "        samples = random.sample(list(self.exemplars.values()),10)[0]\n",
    "        samples = [x for item in samples for x in item]\n",
    "        exemplars=\" \".join(samples)\n",
    "\n",
    "    return applied_rules, exemplars\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    self.data[idx].rules , self.data[idx].exemplars= self.find_rules(idx)\n",
    "    return self.data[idx]\n",
    "\n",
    "  def __iter__(self):\n",
    "    return iter(self.data)\n",
    "\n",
    "  @staticmethod\n",
    "  def collate_fn(samples: List[HateSpeechExample]):\n",
    "    # get the encoding of each thing\n",
    "    # get the labels of each thing\n",
    "    texts = [sample.text for sample in samples]\n",
    "    labels = [sample.label for sample in samples]\n",
    "    exemplars = [sample.exemplars for sample in samples]\n",
    "\n",
    "    text_encoding = HateSpeechDataset.tokenizer(texts,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=512,\n",
    "                                   truncation=True,\n",
    "                                   return_tensors=\"pt\")\n",
    "    exemplar_encoding = HateSpeechDataset.tokenizer(exemplars,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=512,\n",
    "                                   truncation=True,\n",
    "                                   return_tensors=\"pt\")\n",
    "\n",
    "    return {'text_encoding': text_encoding, 'exemplar_encoding': exemplar_encoding, 'labels' : torch.tensor(labels, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rules_path = [\"./CAD/cad_rules.json\", \"./hatexplain/hatexplain_rules.json\", \"./jigsaw/hate_abuse_sample.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(file, tokenizer,  rules, exemplars):\n",
    "  with open(file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "  return HateSpeechDataset(data, tokenizer,  rules, exemplars)\n",
    "\n",
    "def get_rules(rules_path = ['hatexplain_rules.json'], exemplar_path = 'rule_to_exemplar.json'):\n",
    "  rules = []\n",
    "  for rule_set in rules_path:\n",
    "    with open(rule_set, 'r') as f:\n",
    "      new_rules = json.load(f)\n",
    "    rules += new_rules\n",
    "\n",
    "\n",
    "  with open(exemplar_path, 'r') as f:\n",
    "    exemplars = json.load(f)\n",
    "\n",
    "  return rules, exemplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_datasets(tokenizer,  rules, exemplars):\n",
    "  # return a dictionary of train, test, validation datasets\n",
    "  datasets = {}\n",
    "  data_names = ['test', 'train', 'val']\n",
    "  for data_name in data_names:\n",
    "    datasets[data_name] = get_dataset(f'{data_name}.json', tokenizer, rules, exemplars)\n",
    "  return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-zh-cased\")\n",
    "\n",
    "rules, exemplars = get_rules(['hatexplain_rules.json'],'rule_to_exemplar.json')\n",
    "datasets = initialize_datasets(tokenizer, rules, exemplars)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### similarity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(x_e, x_t):\n",
    "  cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "  return cos_sim(x_e, x_t)\n",
    "\n",
    "def get_loss(correct_labels, distance, margin=1e-8):\n",
    "  correct_labels_float = correct_labels.to(dtype=torch.float32)\n",
    "  margin_distance = torch.max(margin - distance, torch.zeros_like(distance))\n",
    "  return torch.mean(0.5*(correct_labels_float * torch.square(distance) + (1-correct_labels_float)*torch.square(margin_distance)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what now?\n",
    "# create a training loop for model\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(exemplar_encoder: nn.Module, text_encoder: nn.Module, dataloader: DataLoader, loss, exemplar_optimizer: Optimizer, text_optimizer: Optimizer, epoch: int, k = 0.5):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    :param model: A pre-trained model loaded from transformers. (e.g., RobertaForSequenceClassification https://huggingface.co/docs/transformers/v4.37.0/en/model_doc/roberta#transformers.RobertaForSequenceClassification)\n",
    "    :param dataloader: A train set dataloader for HateXplain rules and texts.\n",
    "    :param optimizer: An instance of Pytorch optimizer. (e.g., AdamW https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)\n",
    "    :param epoch: An integer denoting current epoch.\n",
    "    Trains model for one epoch.\n",
    "    \"\"\"\n",
    "    exemplar_encoder.train()\n",
    "    text_encoder.train()\n",
    "\n",
    "    with tqdm(dataloader, desc=f\"Train Ep {epoch}\", total=len(dataloader)) as tq:\n",
    "        for batch in tq:\n",
    "            text_encoding = batch['text_encoding'].to(text_encoder.device)\n",
    "\n",
    "            label_encoding = batch['labels'].to(text_encoder.device)\n",
    "            exemplar_encoding = batch['exemplar_encoding'].to(exemplar_encoder.device)\n",
    "\n",
    "            exemplar_output = exemplar_encoder(**exemplar_encoding).last_hidden_state\n",
    "            text_output = text_encoder(**text_encoding).last_hidden_state\n",
    "\n",
    "            t_pooled_output = torch.mean(text_output, dim=1)\n",
    "            e_pooled_output = torch.mean(exemplar_output, dim=1)\n",
    "\n",
    "            similarity = sim(e_pooled_output, t_pooled_output)\n",
    "\n",
    "            loss = get_loss(label_encoding, similarity)\n",
    "\n",
    "            predicted_labels = torch.where(similarity>=k, 1, 0)\n",
    "\n",
    "            exemplar_optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True )\n",
    "            exemplar_optimizer.step()\n",
    "\n",
    "\n",
    "            text_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            text_optimizer.step()\n",
    "\n",
    "            tq.set_postfix({\"loss\": loss.detach().item()}) # for printing better-looking progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(exemplar_encoder: nn.Module, text_encoder: nn.Module, dataloader: DataLoader, loss, k = 0.5) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate model on the dataloader and compute the accuracy.\n",
    "    :param model: A language model loaded from transformers. (e.g., RobertaForSequenceClassification https://huggingface.co/docs/transformers/v4.37.0/en/model_doc/roberta#transformers.RobertaForSequenceClassification)\n",
    "    :param dataloader: A validation / test set dataloader for SST2Dataset\n",
    "    :return: A floating number representing the accuracy of model in the given dataset.\n",
    "    \"\"\"\n",
    "    exemplar_encoder.eval()\n",
    "    text_encoder.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_loss = []\n",
    "    with tqdm(dataloader, desc=f\"Eval\", total=len(dataloader)) as tq:\n",
    "        for batch in tq:\n",
    "            with torch.no_grad():\n",
    "                text_encoding = batch['text_encoding'].to(text_encoder.device)\n",
    "                label_encoding = batch['labels'].to(text_encoder.device)\n",
    "                exemplar_encoding = batch['exemplar_encoding'].to(exemplar_encoder.device)\n",
    "                exemplar_output = exemplar_encoder(**exemplar_encoding).last_hidden_state\n",
    "                text_output = text_encoder(**text_encoding).last_hidden_state\n",
    "\n",
    "                t_pooled_output = torch.mean(text_output, dim=1)\n",
    "                e_pooled_output = torch.mean(exemplar_output, dim=1)\n",
    "\n",
    "                similarity = sim(e_pooled_output, t_pooled_output)\n",
    "                predicted_labels = torch.where(similarity>=k, 1, 0).clone().detach()\n",
    "                loss = get_loss(label_encoding, similarity)\n",
    "\n",
    "                all_predictions += predicted_labels\n",
    "                all_labels += label_encoding\n",
    "                all_loss.append(loss.detach.item())\n",
    "\n",
    "    all_predictions = torch.Tensor(all_predictions)\n",
    "    all_labels = torch.Tensor(all_labels)\n",
    "    accuracy = compute_accuracy(all_predictions, all_labels)\n",
    "    final_loss = all_loss.mean()\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"loss\": final_loss,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_accuracy(predictions: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Given two tensors predictions and labels, compute the accuracy.\n",
    "    :param predictions: torch.Tensor of size (N,)\n",
    "    :param labels: torch.Tensor of size (N,)\n",
    "    :return: A floating number representing the accuracy\n",
    "    \"\"\"\n",
    "    assert predictions.size(-1) == labels.size(-1)\n",
    "\n",
    "    accuracy = torch.mean(1.0 * (predictions == labels))\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "\n",
    "torch.manual_seed(64)\n",
    "\n",
    "def main():\n",
    "    # hyper-parameters (we provide initial set of values here, but you can modify them.)\n",
    "    batch_size = 16\n",
    "    learning_rate = 5e-5\n",
    "    num_epochs = 10\n",
    "    model_name = \"Geotrend/distilbert-base-zh-cased\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "    text_encoder = AutoModel.from_pretrained(model_name).cuda()\n",
    "    exemplar_encoder = AutoModel.from_pretrained(model_name).cuda()\n",
    "\n",
    "    text_optimizer = torch.optim.AdamW(params=text_encoder.parameters(), lr=learning_rate, eps=1e-8)\n",
    "    exemplar_optimizer = torch.optim.AdamW(params=exemplar_encoder.parameters(), lr=learning_rate, eps=1e-8)\n",
    "\n",
    "    rules, exemplars = get_rules(['hatexplain_rules.json'],'rule_to_exemplar.json')\n",
    "\n",
    "    datasets = initialize_datasets(tokenizer, rules, exemplars)\n",
    "\n",
    "    train_dataloader = DataLoader(datasets['train'],\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   collate_fn=HateSpeechDataset.collate_fn,\n",
    "                                   num_workers=2)\n",
    "\n",
    "    validation_dataloader = DataLoader(datasets['val'],\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False,\n",
    "                                   collate_fn=HateSpeechDataset.collate_fn,\n",
    "                                   num_workers=2)\n",
    "\n",
    "    train_acc_history, val_acc_history = [], []\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_one_epoch(exemplar_encoder, text_encoder, train_dataloader, get_loss , exemplar_optimizer, text_optimizer, epoch, k = 0.5)\n",
    "        train_acc = evaluate(exemplar_encoder, text_encoder, train_dataloader, get_loss, k = 0.5)\n",
    "        valid_acc = evaluate(exemplar_encoder, text_encoder, validation_dataloader,get_loss, k = 0.5)\n",
    "\n",
    "        train_acc_history.append(train_acc['accuracy'])\n",
    "        val_acc_history.append(valid_acc['accuracy'])\n",
    "        train_loss_history.append(train_acc['loss'])\n",
    "        val_loss_history.append(valid_acc['loss'])\n",
    "\n",
    "        if valid_acc > best_acc:\n",
    "          torch.save(exemplar_encoder, './checkpoints/best_exemplar_encoder.pth')\n",
    "          torch.save(text_encoder, './checkpoints/best_text_encoder.pth')\n",
    "          best_acc = valid_acc\n",
    "\n",
    "    return train_acc_history, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./checkpoints'):\n",
    "    # If the directory does not exist, create it\n",
    "    os.makedirs('./checkpoints')\n",
    "    print(\"Directory created\")\n",
    "else:\n",
    "    print(\"Directory already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
